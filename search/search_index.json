{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"congressTweets documentation This project aims to develop machine learning models for analyzing tweets from U.S. Senators and Representatives, particularly by extracting and searching based on topics. Dataset The input data for this project consists of approximately 500,000 tweets that have been scraped for each U.S. senator and representative. These tweets form the corpus on which our trained models are applied. Approach 1: Fake Data To identify tweets about some specific topic, e.g. \"broken US healthcare system\", one approach is to construct fake training data: To construct the training dataset, I manually created 20 representative 'fake' tweets that accurately portray the sentiment and language used when discussing broken U.S. healthcare. These tweets were labeled with a 1, signifying that the tweet is about broken U.S. healthcare. We also randomly sampled 20 tweets from the broader corpus that are not related to broken U.S. healthcare, and labeled these tweets with a 0, signifying that they do not pertain to broken U.S. healthcare. In total, the training dataset comprises 40 labeled tweets. These 40 labeled tweets are used to train various machine learning models, which learn to differentiate between tweets about broken U.S. healthcare and those not related to this topic. We apply a range of binary text classification models, including Multinomial Naive Bayes, Support Vector Machine, Logistic Regression, and others. The trained models are then applied to the full corpus of tweets. The model predictions can be evaluated for performance metrics such as precision, recall, and F1-score. From here, you can obtain more training data using an active learning loop and methods such as uncertainty sampling. Approach 2: Embeddings [to be continued, e.g. Doc2Vec, GloVe, FastText, BERT, OpenAI]","title":"Home"},{"location":"#congresstweets-documentation","text":"This project aims to develop machine learning models for analyzing tweets from U.S. Senators and Representatives, particularly by extracting and searching based on topics.","title":"congressTweets documentation"},{"location":"#dataset","text":"The input data for this project consists of approximately 500,000 tweets that have been scraped for each U.S. senator and representative. These tweets form the corpus on which our trained models are applied.","title":"Dataset"},{"location":"#approach-1-fake-data","text":"To identify tweets about some specific topic, e.g. \"broken US healthcare system\", one approach is to construct fake training data: To construct the training dataset, I manually created 20 representative 'fake' tweets that accurately portray the sentiment and language used when discussing broken U.S. healthcare. These tweets were labeled with a 1, signifying that the tweet is about broken U.S. healthcare. We also randomly sampled 20 tweets from the broader corpus that are not related to broken U.S. healthcare, and labeled these tweets with a 0, signifying that they do not pertain to broken U.S. healthcare. In total, the training dataset comprises 40 labeled tweets. These 40 labeled tweets are used to train various machine learning models, which learn to differentiate between tweets about broken U.S. healthcare and those not related to this topic. We apply a range of binary text classification models, including Multinomial Naive Bayes, Support Vector Machine, Logistic Regression, and others. The trained models are then applied to the full corpus of tweets. The model predictions can be evaluated for performance metrics such as precision, recall, and F1-score. From here, you can obtain more training data using an active learning loop and methods such as uncertainty sampling.","title":"Approach 1: Fake Data"},{"location":"#approach-2-embeddings","text":"[to be continued, e.g. Doc2Vec, GloVe, FastText, BERT, OpenAI]","title":"Approach 2: Embeddings"},{"location":"data/tweets/","text":"Tweets Data Sources are latest 1000 tweets for each (1) US Senator and (2) US House Representative, as of May 2023. Scripts: see: scripts/pull_data/ Getting Senator / Representative Handles Scraped from: senators: https://ucsd.libguides.com/congress_twitter/senators representatives: https://ucsd.libguides.com/congress_twitter/reps","title":"tweets"},{"location":"data/tweets/#tweets","text":"Data Sources are latest 1000 tweets for each (1) US Senator and (2) US House Representative, as of May 2023. Scripts: see: scripts/pull_data/","title":"Tweets"},{"location":"data/tweets/#getting-senator-representative-handles","text":"Scraped from: senators: https://ucsd.libguides.com/congress_twitter/senators representatives: https://ucsd.libguides.com/congress_twitter/reps","title":"Getting Senator / Representative Handles"},{"location":"model/preliminary_pipeline/","text":"Preliminary Pipeline import numpy as np from typing import Mapping , List , Any , Optional from dataclasses import dataclass from functools import cached_property # models from sklearn.feature_extraction.text import CountVectorizer , TfidfTransformer from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from sklearn.ensemble import RandomForestClassifier from sklearn.naive_bayes import BernoulliNB # from xgboost import XGBClassifier # pipeline from sklearn.base import BaseEstimator from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.model_selection import GridSearchCV class DummyEstimator ( BaseEstimator ): def fit ( self ): pass def score ( self ): pass @dataclass class Pipe : model : str = \"LogReg\" def __post_init__ ( self ): self . scoring = [ 'accuracy' , 'precision_weighted' , 'recall_weighted' , 'f1_weighted' , 'roc_auc' ] @property def pipe ( self ): return Pipeline ( [ ( 'vect' , CountVectorizer ()), ( 'tfidf' , TfidfTransformer ()), ( 'clf' , DummyEstimator ()) ] ) @cached_property def search_space ( self ): return [ # { # 'clf': [LogisticRegression()], # 'clf__penalty': ['l1','l2'], # 'clf__C': np.logspace(0, 4, 10) # }, { 'clf' : [ SVC ()], 'clf__C' : np . logspace ( 0 , 4 , 10 ) }, { 'clf' : [ KNeighborsClassifier ()], 'clf__n_neighbors' : [ 5 , 10 , 15 ], }, { 'clf' : [ RandomForestClassifier ()], 'clf__max_depth' :[ int ( x ) for x in np . linspace ( 10 , 110 , num = 11 )], 'clf__min_samples_split' :[ 2 , 5 , 10 , 20 ], 'clf__bootstrap' :[ True , False ] }, { 'clf' : [ BernoulliNB ()] } ] @cached_property def search ( self ): return GridSearchCV ( estimator = self . pipe , param_grid = self . search_space , cv = 5 , n_jobs = 10 , scoring = 'accuracy' , refit = \"roc_auc\" , ) def fit ( self , X , y ): return self . search . fit ( X , y )","title":"preliminary pipeline"},{"location":"model/preliminary_pipeline/#preliminary-pipeline","text":"import numpy as np from typing import Mapping , List , Any , Optional from dataclasses import dataclass from functools import cached_property # models from sklearn.feature_extraction.text import CountVectorizer , TfidfTransformer from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from sklearn.ensemble import RandomForestClassifier from sklearn.naive_bayes import BernoulliNB # from xgboost import XGBClassifier # pipeline from sklearn.base import BaseEstimator from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.model_selection import GridSearchCV class DummyEstimator ( BaseEstimator ): def fit ( self ): pass def score ( self ): pass @dataclass class Pipe : model : str = \"LogReg\" def __post_init__ ( self ): self . scoring = [ 'accuracy' , 'precision_weighted' , 'recall_weighted' , 'f1_weighted' , 'roc_auc' ] @property def pipe ( self ): return Pipeline ( [ ( 'vect' , CountVectorizer ()), ( 'tfidf' , TfidfTransformer ()), ( 'clf' , DummyEstimator ()) ] ) @cached_property def search_space ( self ): return [ # { # 'clf': [LogisticRegression()], # 'clf__penalty': ['l1','l2'], # 'clf__C': np.logspace(0, 4, 10) # }, { 'clf' : [ SVC ()], 'clf__C' : np . logspace ( 0 , 4 , 10 ) }, { 'clf' : [ KNeighborsClassifier ()], 'clf__n_neighbors' : [ 5 , 10 , 15 ], }, { 'clf' : [ RandomForestClassifier ()], 'clf__max_depth' :[ int ( x ) for x in np . linspace ( 10 , 110 , num = 11 )], 'clf__min_samples_split' :[ 2 , 5 , 10 , 20 ], 'clf__bootstrap' :[ True , False ] }, { 'clf' : [ BernoulliNB ()] } ] @cached_property def search ( self ): return GridSearchCV ( estimator = self . pipe , param_grid = self . search_space , cv = 5 , n_jobs = 10 , scoring = 'accuracy' , refit = \"roc_auc\" , ) def fit ( self , X , y ): return self . search . fit ( X , y )","title":"Preliminary Pipeline"}]}